{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/Project B/DatasetCondensation-master\n",
        "sys.path.append('/content/drive/MyDrive/Project B/DatasetCondensation-master')"
      ],
      "metadata": {
        "id": "NMGi4TH26b5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yM1GGWYbFeth",
        "outputId": "274c11e1-d83a-4ffc-cab8-fb2079476db0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Project B/DatasetCondensation-master\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import scipy.io as so"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# creating condenced dataset on MNIST and CIFAR10 "
      ],
      "metadata": {
        "id": "dBNdfaU-5zyx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python Main_colab.py --save_path 'result_real_init_MNIST' --num_eval 100 --epoch_eval_train 300 --Iteration 1000 --ipc 10 --dataset MNIST --init real "
      ],
      "metadata": {
        "id": "bZtNIkmZFlfG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python Main_colab.py --save_path 'result_real_init_MNIST' --num_eval 100 --epoch_eval_train 300 --Iteration 1000 --ipc 10 --dataset MNIST --init noise "
      ],
      "metadata": {
        "id": "t1hzE1HF4Wv5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python Main_colab.py --save_path 'result_real_init_CIFAR10' --num_eval 100 --epoch_eval_train 300 --Iteration 1000 --ipc 10 --dataset CIFAR10 --init real "
      ],
      "metadata": {
        "id": "kw4DltVE4YzD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python Main_colab.py --save_path 'result_real_init_CIFAR10' --num_eval 100 --epoch_eval_train 300 --Iteration 1000 --ipc 10 --dataset CIFAR10 --init noise "
      ],
      "metadata": {
        "id": "PbHHKCWj4eHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cross-architecture test"
      ],
      "metadata": {
        "id": "Nkf0CmdB6k76"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python Main_colab.py --save_path 'Result_Cross_architecuture' --num_eval 50 --epoch_eval_train 100 --Iteration 100 --eval_mode M"
      ],
      "metadata": {
        "id": "FhyZJ9qy6kMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating model on the original dataset"
      ],
      "metadata": {
        "id": "XbUgvu4C4zGV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import copy\n",
        "import argparse\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision.utils import save_image\n",
        "from utils import get_loops, get_dataset, get_network, get_eval_pool, evaluate_synset, get_daparam, match_loss, get_time, TensorDataset, epoch, DiffAugment, ParamDiffAug\n",
        "\n"
      ],
      "metadata": {
        "id": "d4K7E60Z5JLu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parser = argparse.ArgumentParser(description='Parameter Processing')\n",
        "parser.add_argument('--dataset', type=str, default='CIFAR10', help='dataset')\n",
        "parser.add_argument('--model', type=str, default='ConvNet', help='model')\n",
        "parser.add_argument('--data_path', type=str, default='data', help='dataset path')\n",
        "parser.add_argument('--save_path', type=str, default='result', help='path to save results')\n",
        "##################################################\n",
        "parser.add_argument(\"-f\", \"--file\", required=False) # this particular code is essential for the parser to work in google colab\n",
        "##################################################\n",
        "args = parser.parse_args()\n",
        "args.device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "3_RaLP6K5Mga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/DIP_Assignment_v5/Project B/DatasetCondensation-master')\n",
        "# from torch.utils.data import Dataset\n",
        "# from torch.utils.data import DataLoader\n",
        "import scipy.io as so"
      ],
      "metadata": {
        "id": "tOL4XYHV5THA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "channel, im_size, num_classes, class_names, mean, std, dst_train, dst_test, testloader = get_dataset(args.dataset, args.data_path)\n",
        "test_loader = torch.utils.data.DataLoader(dst_test, batch_size=256, shuffle=False, num_workers=0)\n",
        "train_loader = torch.utils.data.DataLoader(dst_train, batch_size=256, shuffle=False, num_workers=0)"
      ],
      "metadata": {
        "id": "FNypz9ZM5U4L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_accuracy(model, dataloader):\n",
        "    \"\"\"\n",
        "    computes the accuracy of the model\n",
        "    \"\"\"\n",
        "\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "    accuracy = 0.0\n",
        "    total = 0.0\n",
        "    correct = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data in dataloader:\n",
        "            images, labels = data\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            # TODO\n",
        "            # Return the accuracy\n",
        "            # images = torch.unsqueeze(images,1)\n",
        "            total += images.shape[0]\n",
        "            logits =model(images)\n",
        "            correct += (torch.max(logits, 1)[1].view(labels.size()).data == labels).float().sum()\n",
        "        accuracy = 100*(correct/total)\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "KIs99TSF5YYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_network(args.model, channel, num_classes, im_size).to(args.device) # get a random model\n",
        "model.train()\n",
        "net_parameters = list(model.parameters())\n",
        "optimizer_net = torch.optim.SGD(model.parameters(), lr=args.lr_net)  # optimizer_img for synthetic data\n",
        "device = next(model.parameters()).device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "acc_hist = {'train':[], 'test': []}\n",
        "for epoch in range(200):\n",
        "\n",
        "    ## training step\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        # images = torch.squeeze(images,1)\n",
        "        # TODO\n",
        "        # Follow the step in the tutorial\n",
        "        ## forward + backprop + loss\n",
        "        logits = model(images)\n",
        "        loss = criterion(logits, labels)\n",
        "        optimizer_net.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        ## update model params\n",
        "        optimizer_net.step()\n",
        "        model.eval()\n",
        "        acc_hist['train'].append(get_accuracy(model, train_loader))\n",
        "        acc_hist['test'].append(get_accuracy(model, test_loader))\n",
        "\n",
        "        print('Epoch: %d | Train Accuracy: %.2f | Test Accuracy: %.2f'\\\n",
        "              %(epoch, acc_hist['train'][-1], acc_hist['test'][-1]))"
      ],
      "metadata": {
        "id": "0yyB7FOq5ZsM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}